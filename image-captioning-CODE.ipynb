{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image-captioning-CODE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ikUNW5IfoCS5",
        "61ZNIZYpDBBy",
        "-sDb2Uk6C9YL"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW2NGJX6jJ57"
      },
      "source": [
        "# CSCIS-89 Introduction to Deep Learning\n",
        "\n",
        "# Final Project: \"Image Captioning\" \n",
        "\n",
        "\n",
        "# Nikolas Papadopoulos\n",
        "\n",
        "----------------------------------------\n",
        "\n",
        "References:\n",
        "\n",
        "- https://www.tensorflow.org/text/guide/word_embeddings\n",
        "- https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
        "- https://www.analyticsvidhya.com/blog/2020/11/create-your-own-image-caption-generator-using-keras/\n",
        "- https://medium.com/@raman.shinde15/image-captioning-with-flickr8k-dataset-bleu-4bcba0b52926\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKuVG20gQEKP"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikUNW5IfoCS5"
      },
      "source": [
        "# Load Flickr8k dataset & GloVe embeddings in Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ZNIZYpDBBy"
      },
      "source": [
        "### Flickr8k dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1y2pQ8VGthB"
      },
      "source": [
        "!cd \"drive/My Drive/IntroDL/Final Project\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjclCjxqHT2D"
      },
      "source": [
        "!wget -P \"drive/My Drive/IntroDL/Final Project\" https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmlRw1PRH2Bv"
      },
      "source": [
        "!wget -P \"drive/My Drive/IntroDL/Final Project\" https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFrR059I28J"
      },
      "source": [
        "!mkdir \"drive/My Drive/IntroDL/Final Project/Flickr8k_Dataset\"\n",
        "!unzip \"drive/My Drive/IntroDL/Final Project/Flickr8k_Dataset.zip\" -d \"drive/My Drive/IntroDL/Final Project/Flickr8k_Dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZt-fpKB21JT",
        "outputId": "55be927a-98d5-45ee-e677-8a8cfb9e00df"
      },
      "source": [
        "!find \"drive/My Drive/IntroDL/Final Project/Flickr8k_Dataset/Flicker8k_Dataset\" -type f | sed -e 's/.*\\.//' | sort | uniq -c | sort -n | grep -Ei '(tiff|bmp|jpeg|jpg|png|gif)$'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   8091 jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "410MfZinJRPG"
      },
      "source": [
        "!mkdir \"drive/My Drive/IntroDL/Final Project/Flickr8k_text\"\n",
        "!unzip -qq \"drive/My Drive/IntroDL/Final Project/Flickr8k_text.zip\" -d \"drive/My Drive/IntroDL/Final Project/Flickr8k_text\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_Zjm49JH2M5"
      },
      "source": [
        "!cat \"drive/My Drive/IntroDL/Final Project/Flickr8k_text/Flickr8k.lemma.token.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sDb2Uk6C9YL"
      },
      "source": [
        "### GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxGCkbMpC8e9"
      },
      "source": [
        "!wget -P \"drive/My Drive/IntroDL/Final Project\" http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA8P_SKgDQoj"
      },
      "source": [
        "!unzip -qq '/content/drive/MyDrive/IntroDL/Final Project/glove.6B.zip' -d '/content/drive/MyDrive/IntroDL/Final Project/GloVe'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQMblvTbZRqf"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIXBItxPoKIc"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re2iNlrlJaW5"
      },
      "source": [
        "import numpy as np  \n",
        "import pandas as pd  \n",
        "import os\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\n",
        "from keras.layers import concatenate, BatchNormalization, Input\n",
        "from keras.layers.merge import add\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "import matplotlib.pyplot as plt \n",
        "import cv2\n",
        "\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "import string\n",
        "# import os\n",
        "# import glob\n",
        "# from PIL import Image\n",
        "# from time import time\n",
        "\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jocJHQNPZYSR"
      },
      "source": [
        "### Define directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJvEbYL7G0LI"
      },
      "source": [
        "token_path = 'drive/My Drive/IntroDL/Final Project/Flickr8k_text/Flickr8k.token.txt'\n",
        "train_images_path = 'drive/My Drive/IntroDL/Final Project/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "test_images_path = 'drive/My Drive/IntroDL/Final Project/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "images_path = 'drive/My Drive/IntroDL/Final Project/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
        "glove_path = '/content/drive/MyDrive/IntroDL/Final Project/GloVe/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK2kSp-Os0pv"
      },
      "source": [
        "### Create vocabulary and text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mltpXRGLJaec"
      },
      "source": [
        "'''\n",
        "Create a dictionary that stores as key the image_id and \n",
        "as values the 5 descriptions for each image\n",
        "'''\n",
        "def load_description(doc):\n",
        "  mapping = dict()\n",
        "  for line in doc.split('\\n'):\n",
        "    token = line.split('\\t')\n",
        "    if len(line) > 2: # remove short descriptions\n",
        "      image_id = token[0].split('.')[0]\n",
        "      image_description = token[1].split('.')[0]\n",
        "      if image_id not in mapping:\n",
        "        mapping[image_id] = list()\n",
        "      mapping[image_id].append(image_description)\n",
        "  return mapping\n",
        "\n",
        "\n",
        "'''\n",
        "Clean the text of descriptions by removing special characters \n",
        "(eg. symbols and numbers) and uncapitalizing text\n",
        "'''\n",
        "def clean_description(desc):\n",
        "  for key, desc_list in desc.items():\n",
        "    for i in range(len(desc_list)):\n",
        "      caption = desc_list[i]\n",
        "      # remove punctuation\n",
        "      caption = [ch for ch in caption if ch not in string.punctuation] \n",
        "      caption = ''.join(caption)\n",
        "      caption = caption.split(' ')\n",
        "      # uncapitalize words\n",
        "      caption = [word.lower() for word in caption if word.isalpha()] \n",
        "      caption = ' '.join(caption)\n",
        "      desc_list[i] = caption \n",
        "\n",
        "\n",
        "'''\n",
        "Create a vocabulary of all unique words existing in our corpus\n",
        "'''\n",
        "def create_vocabulary(desc):\n",
        "\tvocabulary = set()\n",
        "\tfor key in desc.keys():\n",
        "\t\tfor line in desc[key]:\n",
        "\t\t\tvocabulary.update(line.split())\n",
        "\treturn vocabulary\n",
        "\n",
        "'''\n",
        "Print images from the dataset using the image id\n",
        "'''\n",
        "def print_image(id):\n",
        "  x = plt.imread(images_path + id + '.jpg')\n",
        "  plt.imshow(x)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZnbf0rSXEdN"
      },
      "source": [
        "print('\\n **** Plot an example image: **** \\n')\n",
        "\n",
        "print_image('1000268201_693b08cb0e')\n",
        "\n",
        "print('\\n=================\\n')\n",
        "\n",
        "print('\\n **** Print a part of the corpus: **** \\n')\n",
        "\n",
        "corpus = open(token_path,'r').read()\n",
        "print(corpus[:410])\n",
        "\n",
        "print('\\n=================\\n')\n",
        "\n",
        "print('\\n **** Print descriptions of an image after the creation of the dictionary: **** \\n')\n",
        "\n",
        "descriptions = load_description(corpus)\n",
        "for i in descriptions['1000268201_693b08cb0e']:\n",
        "  print(i)\n",
        "\n",
        "print('\\n=================\\n')\n",
        "\n",
        "print('\\n **** Print descriptions of the same image after cleaning the text: **** \\n')\n",
        "\n",
        "clean_description(descriptions)\n",
        "for i in descriptions['1000268201_693b08cb0e']:\n",
        "  print(i)\n",
        "\n",
        "print('\\n=================\\n')\n",
        "\n",
        "print('\\n **** Print vocabulary size: **** \\n')\n",
        "\n",
        "vocabulary = create_vocabulary(descriptions)\n",
        "print('We have {} words in our vocabulary'.format(len(vocabulary)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UboiCKhMtH_W"
      },
      "source": [
        "### Create a list of ids of all train and test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDvT8-DTJSPT"
      },
      "source": [
        "'''\n",
        "Create a list of ids of all train and test images and \n",
        "then create an empty dictionary and map the images \n",
        "to their descriptions using image id as key and \n",
        "a list of descriptions as its value.\n",
        "'''\n",
        "\n",
        "train_images = open(train_images_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\n",
        "train_images = [image.split('.')[0] for image in train_images] # throw the '.jpg' suffix from each image id\n",
        "train_images = train_images[:-1] # throw the last element which is an empty string\n",
        "\n",
        "test_images = open(test_images_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\n",
        "test_images = [image.split('.')[0] for image in test_images] # throw the '.jpg' suffix from each image id\n",
        "test_images = test_images[:-1] # throw the last element which is an empty string\n",
        "\n",
        "print('There are {} train images and {} test images'.format(len(train_images), len(test_images)))\n",
        "\n",
        "\n",
        "def load_clean_descriptions(des, dataset):\n",
        "    dataset_des = dict()\n",
        "    for key, des_list in des.items():\n",
        "        if key  in dataset:\n",
        "            if key not in dataset_des:\n",
        "                dataset_des[key] = list()\n",
        "            for line in des_list:\n",
        "                desc = 'startseq ' + line + ' endseq' # add unique words at the beginning and end\n",
        "                dataset_des[key].append(desc)\n",
        "    return dataset_des\n",
        "\n",
        "\n",
        "train_descriptions = load_clean_descriptions(descriptions, train_images)\n",
        "test_descriptions = load_clean_descriptions(descriptions, test_images)\n",
        "\n",
        "print(train_descriptions['2205328215_3ffc094cde'])\n",
        "print(test_descriptions['2447284966_d6bbdb4b6e'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt2Y3FA_5r84"
      },
      "source": [
        "### Preprocess caption before Glove embedding extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfAqyrsGaLpn"
      },
      "source": [
        "# list of all training captions\n",
        "all_train_captions = []\n",
        "for key, val in train_descriptions.items():\n",
        "    for caption in val:\n",
        "        all_train_captions.append(caption)\n",
        "\n",
        "# find the maximum length of a description in a dataset\n",
        "max_length = max(len(des.split()) for des in all_train_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAvuGAaL3TyR"
      },
      "source": [
        "print(len(all_train_captions), max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzS4kYgc3nVS"
      },
      "source": [
        "# Consider only words which occur at least 10 times\n",
        "\n",
        "threshold = 10 # you can change this value according to your need\n",
        "word_counts = {}\n",
        "for cap in all_train_captions:\n",
        "    for word in cap.split(' '):\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "  \n",
        "voc = [word for word in word_counts if word_counts[word] >= threshold]\n",
        "\n",
        "print('Vocabulary = %d' % (len(voc)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ9zXR7i5wTo"
      },
      "source": [
        "# word mapping to integers\n",
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "  \n",
        "ix = 1\n",
        "for word in voc:\n",
        "    wordtoix[word] = ix\n",
        "    ixtoword[ix] = word\n",
        "    ix += 1\n",
        "\n",
        "vocab_size = len(ixtoword) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGnUXIU85wWn"
      },
      "source": [
        "# find the maximum length of a description in a dataset\n",
        "max_length = max(len(des.split()) for des in all_train_captions)\n",
        "print(max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ48OMLhJ3pP"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open(glove_path + 'glove.6B.200d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found {} word vectors.\".format(len(embeddings_index)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uv5enPcAMCS"
      },
      "source": [
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in wordtoix.items():\n",
        "    emb_vec = embeddings_index.get(word)\n",
        "    if emb_vec is not None:\n",
        "        embedding_matrix[i] = emb_vec\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526Dl4WrPGWT"
      },
      "source": [
        "### Load the feature vector from all images\n",
        "\n",
        "Load feature vector from pickle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yRyqrREPKhs"
      },
      "source": [
        "train_file = open(\"drive/My Drive/IntroDL/Final Project/encoding_train.pkl\", \"rb\")\n",
        "encoding_train = pickle.load(train_file)\n",
        "\n",
        "test_file = open(\"drive/My Drive/IntroDL/Final Project/encoding_test.pkl\", \"rb\")\n",
        "encoding_test = pickle.load(test_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogIe3vpE3Fxw"
      },
      "source": [
        "### Extract the feature vector from all images\n",
        "\n",
        "Uncomment in case you want to extract feature vectors for an image dataset. Keep in mind that for Flickr8k data, which has 8000 images, feature extraction lasted almost an hour using Colab's GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT6SVISPI-gD"
      },
      "source": [
        "# def preprocess_img(img_path):\n",
        "#     # inception v3 accept images of size 299 * 299 * 3\n",
        "#     img = load_img(img_path, target_size = (299, 299))\n",
        "#     x = img_to_array(img)\n",
        "#     x = np.expand_dims(x, axis = 0) # Add one more dimension\n",
        "#     x = preprocess_input(x)\n",
        "#     return x\n",
        "  \n",
        "# def encode(image):\n",
        "#     image = preprocess_img(images_path + image + '.jpg')\n",
        "#     vec = model.predict(image)\n",
        "#     vec = np.reshape(vec, (vec.shape[1]))\n",
        "#     return vec\n",
        "  \n",
        "# base_model = InceptionV3(weights = 'imagenet')\n",
        "# model = Model(base_model.input, base_model.layers[-2].output)\n",
        "\n",
        "# # Run the encode function on all train and test images\n",
        "# # and store the feature vectors in a list\n",
        "# encoding_train = {}\n",
        "# for img in train_images:\n",
        "#   encoding_train[img] = encode(img)\n",
        "\n",
        "# encoding_test = {}\n",
        "# for img in test_images:\n",
        "#     encoding_test[img] = encode(img)\n",
        "\n",
        "# # save the file \n",
        "# pickle.dump(encoding_train, open(\"drive/My Drive/IntroDL/Final Project/encoding_train.pkl\",\"wb\"))\n",
        "# # save the file \n",
        "# pickle.dump(encoding_test, open(\"drive/My Drive/IntroDL/Final Project/encoding_test.pkl\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UZZ4j5nZ3IJ"
      },
      "source": [
        "# Define and train models\n",
        "\n",
        "### Uncomment this section in case you need to train a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgVTVNFzlGCT"
      },
      "source": [
        "### Define the Model\n",
        "\n",
        "We are creating a Merge model where we combine the image vector and the partial caption. Therefore our model will have 3 major steps:\n",
        "\n",
        "1. Processing the sequence from the text\n",
        "2. Extracting the feature vector from the image\n",
        "3. Decoding the output using softmax by concatenating the above two layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8O_nFIGk4d0",
        "outputId": "7eebe722-7ce1-4e35-a37b-5c38d3a08c94"
      },
      "source": [
        "# # Define the model\n",
        "# input1 = Input(shape = (2048, ))\n",
        "# feature1 = Dropout(0.2)(input1)\n",
        "# feature2 = Dense(256, activation = 'relu')(feature1)\n",
        "\n",
        "# input2 = Input(shape = (max_length, ))\n",
        "# sequence1 = Embedding(vocab_size, embedding_dim, mask_zero = True)(input2)\n",
        "# sequence2 = Dropout(0.2)(sequence1)\n",
        "# sequence3 = LSTM(256)(sequence2)\n",
        "\n",
        "# decoder1 = add([feature2, sequence3])\n",
        "# decoder2 = Dense(256, activation = 'relu')(decoder1)\n",
        "# outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n",
        "\n",
        "# model = Model(inputs = [input1, input2], outputs = outputs)\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 37)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 37, 200)      330800      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 37, 200)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1654)         425078      dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,814,182\n",
            "Trainable params: 1,814,182\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSCRqP8ip7vs"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee2OuNNKk4gn"
      },
      "source": [
        "# model.layers[2].set_weights([embedding_matrix])\n",
        "# model.layers[2].trainable = False\n",
        "# model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhFc3mhsk4js"
      },
      "source": [
        "# def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "#     X1, X2, y = list(), list(), list()\n",
        "#     n = 0\n",
        "#     # loop for ever over images\n",
        "#     while 1:\n",
        "#         for key, desc_list in descriptions.items():\n",
        "#             n += 1\n",
        "#             # retrieve the photo feature\n",
        "#             photo = photos[key]\n",
        "#             for desc in desc_list:\n",
        "#                 # encode the sequence\n",
        "#                 seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "#                 # split one sequence into multiple X, y pairs\n",
        "#                 for i in range(1, len(seq)):\n",
        "#                     # split into input and output pair\n",
        "#                     in_seq, out_seq = seq[:i], seq[i]\n",
        "#                     # pad input sequence\n",
        "#                     in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "#                     # encode output sequence\n",
        "#                     out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "#                     # store\n",
        "#                     X1.append(photo)\n",
        "#                     X2.append(in_seq)\n",
        "#                     y.append(out_seq)\n",
        "\n",
        "#             if n == num_photos_per_batch:\n",
        "#                 yield ([array(X1), array(X2)], array(y))\n",
        "#                 X1, X2, y = list(), list(), list()\n",
        "#                 n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdsrY0dEk4ml"
      },
      "source": [
        "# epochs = 30\n",
        "# batch_size = 128\n",
        "# steps = len(train_descriptions) // batch_size\n",
        "# train_features = encoding_train\n",
        "\n",
        "# generator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\n",
        "# history = model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "# model.save('drive/My Drive/IntroDL/Final Project/model_{}.h5'.format(epochs))  # creates a HDF5 file with the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyb-eUZAaB1E"
      },
      "source": [
        "# Load already trained models\n",
        "\n",
        "### Load weights from already trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw3AOhMwaFTe"
      },
      "source": [
        "epochs = 30\n",
        "model = load_model('drive/My Drive/IntroDL/Final Project/model_{}.h5'.format(epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlXt7JHXxYdA"
      },
      "source": [
        "history_file = open(\"drive/My Drive/IntroDL/Final Project/history_model_{}.pkl\".format(epochs), \"rb\")\n",
        "history = pickle.load(history_file)\n",
        "\n",
        "loss = history['loss']\n",
        "\n",
        "epoch_range = range(len(loss))\n",
        "\n",
        "plt.plot(epoch_range, loss, label='Training loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VoCHWc-Zvo-"
      },
      "source": [
        "# Generate captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GnnEY-pk4sZ"
      },
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0kNdsbk4vo"
      },
      "source": [
        "def beam_search_predictions(image, beam_index = 3):\n",
        "    start = [wordtoix[\"startseq\"]]\n",
        "    start_word = [[start, 0.0]]\n",
        "    while len(start_word[0][0]) < max_length:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "            preds = model.predict([image,par_caps], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "            # Getting the top <beam_index>(n) predictions and creating a \n",
        "            # new list so as to put them via the model again\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "                    \n",
        "        start_word = temp\n",
        "        # Sorting according to the probabilities\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        # Getting the top words\n",
        "        start_word = start_word[-beam_index:]\n",
        "    \n",
        "    start_word = start_word[-1][0]\n",
        "    intermediate_caption = [ixtoword[i] for i in start_word]\n",
        "    final_caption = []\n",
        "    \n",
        "    for i in intermediate_caption:\n",
        "        if i != 'endseq':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpFCDKkXYiRt"
      },
      "source": [
        "encoding_test_keys = list(encoding_test.keys())[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Gce0slk4yf"
      },
      "source": [
        "encoding_test_keys = list(encoding_test.keys())[:5] # take first 5 keys of encoding_test dictionary\n",
        "\n",
        "for key in encoding_test_keys:\n",
        "  image = encoding_test[key].reshape((1,2048))\n",
        "  x = plt.imread(images_path + key + '.jpg')\n",
        "  plt.imshow(x)\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\nGreedy Search:\", greedySearch(image))\n",
        "  print(\"Beam Search, K = 3:\", beam_search_predictions(image, beam_index = 3))\n",
        "  print(\"Beam Search, K = 5:\", beam_search_predictions(image, beam_index = 5))\n",
        "  print(\"Beam Search, K = 7:\", beam_search_predictions(image, beam_index = 7))\n",
        "  print(\"Beam Search, K = 10:\", beam_search_predictions(image, beam_index = 10))\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srm8JUL0FphV"
      },
      "source": [
        "# Generate caption for new images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLs1SOfKH6YY"
      },
      "source": [
        "def preprocess_img(img_path):\n",
        "    # inception v3 accept images of size 299 * 299 * 3\n",
        "    img = load_img(img_path, target_size = (299, 299))\n",
        "    x = img_to_array(img)\n",
        "    x = np.expand_dims(x, axis = 0) # Add one more dimension\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "  \n",
        "def encode(image, img_path):\n",
        "    image = preprocess_img(img_path + image + '.jpg')\n",
        "    vec = inception_model.predict(image)\n",
        "    vec = np.reshape(vec, (vec.shape[1]))\n",
        "    return vec\n",
        "  \n",
        "base_model = InceptionV3(weights = 'imagenet')\n",
        "inception_model = Model(base_model.input, base_model.layers[-2].output)\n",
        "\n",
        "# Run the encode function on new images\n",
        "# and store the feature vectors in a list\n",
        "new_images_path = 'drive/My Drive/IntroDL/Final Project/New_images/'\n",
        "# img = 'giannis-antetokounmpo'\n",
        "# img = 'antetokounmpo-shoot'\n",
        "# img = 'street-basketball'\n",
        "# img = 'nadal'\n",
        "img = 'boy-crying'\n",
        "encoding_new_images = {}\n",
        "encoding_new_images[img] = encode(img, new_images_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk4AafSHL1qh"
      },
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1txxe0zYI7MQ"
      },
      "source": [
        "def beam_search_predictions(image, beam_index = 3):\n",
        "    start = [wordtoix[\"startseq\"]]\n",
        "    start_word = [[start, 0.0]]\n",
        "    while len(start_word[0][0]) < max_length:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "            preds = model.predict([image,par_caps], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "            # Getting the top <beam_index>(n) predictions and creating a \n",
        "            # new list so as to put them via the model again\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "                    \n",
        "        start_word = temp\n",
        "        # Sorting according to the probabilities\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        # Getting the top words\n",
        "        start_word = start_word[-beam_index:]\n",
        "    \n",
        "    start_word = start_word[-1][0]\n",
        "    intermediate_caption = [ixtoword[i] for i in start_word]\n",
        "    final_caption = []\n",
        "    \n",
        "    for i in intermediate_caption:\n",
        "        if i != 'endseq':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59qEzOkgL8dK"
      },
      "source": [
        "# pic = 'street-basketball'\n",
        "# pic = 'giannis-antetokounmpo'\n",
        "# pic = 'antetokounmpo-shoot'\n",
        "# pic = 'nadal'\n",
        "pic = 'boy-crying'\n",
        "image = encoding_new_images[pic].reshape((1,2048))\n",
        "x = plt.imread(new_images_path + pic + '.jpg')\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nGreedy Search:\", greedySearch(image))\n",
        "print(\"Beam Search, K = 3:\", beam_search_predictions(image, beam_index = 3))\n",
        "print(\"Beam Search, K = 5:\", beam_search_predictions(image, beam_index = 5))\n",
        "print(\"Beam Search, K = 7:\", beam_search_predictions(image, beam_index = 7))\n",
        "print(\"Beam Search, K = 10:\", beam_search_predictions(image, beam_index = 10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}